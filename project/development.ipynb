{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ðŸ”¥ _DeepFire_: API Project for Fire Detection ðŸ”¥\n",
    "\n",
    "In this project, you'll apply your skills at neural network development in a new way: taking a model that you've trained yourself and deploying it to a static webpage that you can work with to upload new images and get prediction accuracy results. \n",
    "\n",
    "This project will primarily focus on your abilities in creating and testing neural network architecture development. \n",
    "\n",
    "#### **Specifically, you'll be creating a convolutional neural network that can ingest Fire Detection Image Data and predict binary class values, similarly to what we've done with multilayer perceptrons in the past.**\n",
    "\n",
    "Boilerplate and supporting architectures have been provided for a multitude of tasks ranging from data preprocessing, processing, ingestion, and predictive assessment â€“Â however, major tasks and design work will ultimately be left to you to approach and figure out ideal, optimized solutions. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ðŸ”¹ General Importations\n",
    "\n",
    "As always, we'll start with importing basic tools and functions for our task."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import utils\n",
    "\n",
    "import os, PIL\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž Initializing Deep Learning Tools ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> Your first task will be crucial to ensuring the successful implementation of the rest of your notebook. \n",
    "> \n",
    "> **Initialize each line with the correct function type from the TensorFlow documentation.**\n",
    "> \n",
    "> Feel free to refer throughout the notebook and across previous notebooks to see which TensorFlow architectures you've used for similar tasks. \n",
    "> \n",
    "> To give you a guide for how this should look, you've been provided with a single correct function declaration in the form of `image_dataset_from_directory` at the end of the cell. \n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\"\"\" Sequential Model Architecture \"\"\"\n",
    "Sequential = tf.keras.models.Sequential\n",
    "\n",
    "\"\"\" Data Preprocessing Functions \"\"\"\n",
    "Resizing = tf.keras.layers.Resizing\n",
    "Rescaling = tf.keras.layers.Rescaling\n",
    "\n",
    "\"\"\" Data Augmentation Functions \"\"\"\n",
    "RandomFlip = tf.keras.layers.RandomFlip\n",
    "RandomRotation = tf.keras.layers.RandomRotation\n",
    "RandomZoom = tf.keras.layers.RandomZoom\n",
    "\n",
    "\"\"\" Artificial Neural Network Layer Inventory \"\"\"\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "\n",
    "\"\"\" Convolutional Neural Network Layer Inventory \"\"\"\n",
    "Conv2D = tf.keras.layers.Conv2D\n",
    "MaxPool2D = tf.keras.layers.MaxPool2D\n",
    "Flatten = tf.keras.layers.Flatten\n",
    "\n",
    "\"\"\" Residual Network Layer Inventory \"\"\"\n",
    "ResNet50 = tf.keras.applications.resnet50.ResNet50\n",
    "\n",
    "\"\"\" Function to Load Images from Target Folder \"\"\"\n",
    "image_dataset_from_directory = tf.keras.preprocessing.image_dataset_from_directory"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ðŸ”¹ Precheck Image Dataset Sizes\n",
    "\n",
    "If you've followed instructions carefully from the `project/PROJECT.md` instructions, the following dataset directory instantiations should work perfectly. \n",
    "\n",
    "If they do not, double-check to make sure you've saved your dataset to the appropriate location. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Use the `glob.glob` function to show how many images are in each folder\n",
    "DATA_DIRECTORY = \"../dataset/Images/\"\n",
    "FIRE_IMAGES_PATTERN = f\"{DATA_DIRECTORY}/Fire_Images/*\"\n",
    "NOT_FIRE_IMAGES_PATTERN = f\"{DATA_DIRECTORY}/Normal_Images/*\"\n",
    "\n",
    "print(f\"Number of fire image samples: {len(glob(FIRE_IMAGES_PATTERN))}\")\n",
    "print(f\"Number of non-fire image samples: {len(glob(NOT_FIRE_IMAGES_PATTERN))}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of fire image samples: 110\n",
      "Number of non-fire image samples: 541\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ðŸ”¸ Load Dataset\n",
    "\n",
    "Like we've done previously, let's set our batch size and image dimensions to work seamlessly with our configured model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "batch_size = 32\n",
    "IMAGE_HEIGHT = IMAGE_WIDTH = 256"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "train = image_dataset_from_directory(\n",
    "    directory=DATA_DIRECTORY,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=batch_size\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 651 files belonging to 2 classes.\n",
      "Using 521 files for training.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class_names = train.class_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "validation = image_dataset_from_directory(\n",
    "    directory=DATA_DIRECTORY,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=batch_size\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 651 files belonging to 2 classes.\n",
      "Using 130 files for validation.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From these results, we can actually see that we have a major class imbalance with our fire images representing our minority class. \n",
    "\n",
    "Let's go ahead and fix that by resampling our dataset. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ðŸ”¹ Resample (Oversample) Minority Class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def resample_class_distribution(train, DATA_DIRECTORY=DATA_DIRECTORY, save=True):\n",
    "    \"\"\" Helper function to resample class distribution for image dataset. \"\"\"\n",
    "    minority_class, majority_class = list(), list()\n",
    "    for images, labels, in train.take(3):\n",
    "        for image, label in zip(images, labels):\n",
    "            if label == 0:\n",
    "                minority_class.append(image.numpy().astype(np.uint8))\n",
    "            else:\n",
    "                majority_class.append(image.numpy().astype(np.uint8))\n",
    "    FIRE_SIZE = len(glob(f\"{DATA_DIRECTORY}/Fire_Images/*\"))\n",
    "    NOT_FIRE_SIZE = len(glob(f\"{DATA_DIRECTORY}/Normal_Images/*\"))\n",
    "    upsampled_images = np.array(utils.resample(minority_class, replace=True, \n",
    "                                               n_samples=(NOT_FIRE_SIZE - FIRE_SIZE),\n",
    "                                               random_state=42))\n",
    "    if save == True:\n",
    "        index = 0\n",
    "        for image in upsampled_images:\n",
    "            PATH = f\"{DATA_DIRECTORY}/Fire_Images/new_fire_{index}.png\"\n",
    "            PIL.Image.fromarray(image).save(PATH)\n",
    "            index += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "resample_class_distribution(train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should now see that additional images have been generated to balance out both classes prior to predictive modeling.\n",
    "\n",
    "**Go ahead and re-run the `Load Dataset` steps to see new generated dataset changes.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ðŸ”¹ Pre-Optimize Image File Ingestion\n",
    "\n",
    "This is an accessory step to optimize image data ingestion at the cost of slightly higher memory usage. No modifications are required for this function. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def configure_performant_datasets(dataset, shuffling=None):\n",
    "    \"\"\" \n",
    "    Custom function to prefetch and cache stored elements\n",
    "    of retrieved image data to boost latency and performance\n",
    "    at the cost of higher memory usage. \n",
    "    \"\"\"\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    # Cache and prefetch elements of input data for boosted performance\n",
    "    if not shuffling:\n",
    "        return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    else:\n",
    "        return dataset.cache().shuffle(shuffling).prefetch(buffer_size=AUTOTUNE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "train =         configure_performant_datasets(train, shuffling=1000)\n",
    "validation =    configure_performant_datasets(validation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž Create Resizing and Normalization Layers ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> For this task, you'll declare your resizing and normalization layers using the layer architectures that you imported earlier. \n",
    ">\n",
    "> Recall that for this step, we want to accomplish two key tasks: \n",
    "> - Resize all images to the predetermined square image dimensions as indicated by `IMAGE_HEIGHT` and `IMAGE_WIDTH`.\n",
    "> - Scale all images so pixel values are within the range of (0., 1.) rather than the original (0., 255.).\n",
    ">\n",
    "> Additionally, since we're working with colorized image data, we'll want to ensure that our image rescaling/normalization step inputs images as stacks-of-three, since each image channel corresponds to red, green, and blue pixel values. \n",
    ">\n",
    "> As always, refer to previous notebook documentation on image normalization for colorization if you need help.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "resizing_layer = Resizing(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "normalization_layer = Rescaling(1./255, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž Neural Network Architecture Creation ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> Now time for the main event! \n",
    "> \n",
    "> Here, you'll be creating and instantiating your model architecture. \n",
    "> \n",
    "> For this assignment, you'll be creating a **Convolutional Neural Network** that can process Fire Detection images for predictive purposes.\n",
    "> \n",
    "> _For this project, you will not be provided guidance as to how to design and implement your CNN architecture._\n",
    "> \n",
    "> Refer to previous notebooks and challenges on CNNs as well as online documentation/resources for how to design CNN models on higher-order images. \n",
    "> \n",
    "> **This is a highly creative step, and there are no wrong answers; however, you will be assessed on your experimentation process and why you choose specific modeling layers, configurations, optimizers, regularizers, and overall design choices.**\n",
    ">\n",
    "> Light boilerplate will be provided to get you started, but as always, use any and all resources at your disposal to finish the job! \n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "\"\"\" Sequential Model Architecture Setup \"\"\"\n",
    "model = Sequential()\n",
    "\n",
    "\"\"\" CNN Layering Steps \"\"\"\n",
    "# TODO: Instantiate and Add Layers to Create a Functional CNN for Image Recognition.\n",
    "# TODO: Be Sure to Use as Many Imported Layers at the Top of the Notebook as Possible. \n",
    "input_layer = tf.keras.layers.InputLayer(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "\n",
    "random_flipping_layer = RandomFlip(\n",
    "    \"horizontal\", \n",
    "    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "random_rotation_layer = RandomRotation(0.1)\n",
    "random_zooming_layer = RandomZoom(0.1)\n",
    "\n",
    "augmentation_layer = Sequential([random_flipping_layer, random_rotation_layer, random_zooming_layer])\n",
    "\n",
    "conv_layer = Conv2D(filters=8, kernel_size=3, padding=\"same\", activation=\"relu\")\n",
    "conv_layer_2 = Conv2D(filters=16, kernel_size=3, padding=\"same\", activation=\"relu\")\n",
    "\n",
    "pooling_layer = MaxPool2D(pool_size=2)\n",
    "\n",
    "dropout_layer = Dropout(0.2)\n",
    "\n",
    "flatten_layer = Flatten()\n",
    "\n",
    "dense_layer_1 = Dense(128, activation=\"relu\")\n",
    "dense_layer_2 = Dense(64, activation=\"relu\")\n",
    "output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "model.add(input_layer)\n",
    "model.add(augmentation_layer)\n",
    "model.add(resizing_layer)\n",
    "model.add(normalization_layer)\n",
    "model.add(conv_layer)\n",
    "model.add(pooling_layer)\n",
    "model.add(conv_layer_2)\n",
    "model.add(pooling_layer)\n",
    "model.add(flatten_layer)\n",
    "model.add(dense_layer_1)\n",
    "model.add(dropout_layer)\n",
    "model.add(dense_layer_2)\n",
    "model.add(output_layer)\n",
    "\n",
    "\"\"\" CNN Architecture Summarization \"\"\"\n",
    "# TODO: Save Screenshots of your Model Summaries to put in this Project Subfolder.\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_3 (Sequential)   (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " resizing (Resizing)         (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " rescaling (Rescaling)       (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 256, 256, 8)       224       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 128, 128, 16)      1168      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 65536)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               8388736   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,398,449\n",
      "Trainable params: 8,398,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž Neural Network Configuration ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> For this task, you'll compile your CNN architecture with appropriate parameters for loss calculation, optimization, and accuracy metrics.\n",
    "> \n",
    "> As always, refer to previous notebooks, tutorials, and documentation for best-case parameters to use for image recognition models.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "\"\"\" CNN Model Compilation \"\"\"\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"Adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž CNN Model Predictive Fitness ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> For this task, you'll be taking your compiled model and fitting it against your training and validation data.\n",
    "> \n",
    "> Keep in mind that there are several opportunities for further optimizing your workflow with techniques such as batch normalization, generator-based data feeding, etc. \n",
    "> \n",
    "> As always, refer to previous notebooks, tutorials, and documentation for designing model fitness with validation data. \n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "\"\"\" CNN Model Fitness and History Extraction \"\"\"\n",
    "epochs = 10\n",
    "history = model.fit(train, validation_data=validation,\n",
    "                    epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "17/17 [==============================] - 12s 466ms/step - loss: 0.5830 - accuracy: 0.8061 - val_loss: 0.3993 - val_accuracy: 0.8538\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 7s 410ms/step - loss: 0.3643 - accuracy: 0.8426 - val_loss: 0.3398 - val_accuracy: 0.8308\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 7s 393ms/step - loss: 0.3193 - accuracy: 0.8637 - val_loss: 0.3117 - val_accuracy: 0.8769\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 7s 383ms/step - loss: 0.2652 - accuracy: 0.8944 - val_loss: 0.2662 - val_accuracy: 0.8846\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 7s 393ms/step - loss: 0.2539 - accuracy: 0.8887 - val_loss: 0.3070 - val_accuracy: 0.8923\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 6s 380ms/step - loss: 0.2401 - accuracy: 0.9002 - val_loss: 0.2749 - val_accuracy: 0.8923\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 7s 383ms/step - loss: 0.2095 - accuracy: 0.9021 - val_loss: 0.2486 - val_accuracy: 0.8923\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 7s 402ms/step - loss: 0.2352 - accuracy: 0.9079 - val_loss: 0.2209 - val_accuracy: 0.9154\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 13s 791ms/step - loss: 0.2215 - accuracy: 0.9098 - val_loss: 0.2856 - val_accuracy: 0.9077\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 10s 569ms/step - loss: 0.1820 - accuracy: 0.9117 - val_loss: 0.2362 - val_accuracy: 0.9154\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž CNN Model Evaluation ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> For this task, you'll evaluate your CNN model using the validation dataset in order to calculate overall validation accuracy and loss.\n",
    "> \n",
    "> As always, refer to previous notebooks, tutorials, and documentation for using the proper evaluation function for model prediction. \n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "\"\"\" CNN Model Predictive Evaluation \"\"\"\n",
    "# TODO: Evaluate Model Against Validation Dataset\n",
    "model.evaluate(validation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5/5 [==============================] - 1s 77ms/step - loss: 0.2362 - accuracy: 0.9154\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.23623475432395935, 0.9153845906257629]"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ðŸ”¹ Model State Saving\n",
    "\n",
    "When you are satisfied with your model state configuration and performance and are ready to export the model's weights and parameters for deployment purposes, simply run the following function! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def save_model(model, file_name, save_format):\n",
    "    \"\"\" \n",
    "    Save the model weights and architecture.\n",
    "    \n",
    "    Parameters: \n",
    "       model(Model): keras Model object being saved\n",
    "       file_name(str): name of the Hadoop file where\n",
    "                       the whole model will be saved\n",
    "       save_format(str): Indicates whether to save the model to the default\n",
    "                         SavedModel('tf'), or HDF5('h5'), or \n",
    "                         use both H5 and JSON ('composite') formats. \n",
    "       Returns: None\n",
    "    \"\"\"\n",
    "    MODEL_DIRECTORY = \"../model\"\n",
    "    def __save_as_composite():\n",
    "      \"\"\" Saving the model as H5 (for params) + JSON (for the architecture) \"\"\"\n",
    "      # Save the weights\n",
    "      model.save_weights(f'{MODEL_DIRECTORY}/{file_name}_params.h5')\n",
    "      # Save the architecture\n",
    "      with open(f'{MODEL_DIRECTORY}/{file_name}_layers.json', 'w') as f:\n",
    "          f.write(model.to_json())\n",
    "    \n",
    "    def __save_as_h5():\n",
    "      \"\"\" Option 2: Saving whole model as a single H5 file (more storage) \"\"\"\n",
    "      model.save(f\"{MODEL_DIRECTORY}/{file_name}.h5\", save_format=save_format)\n",
    "\n",
    "    # Call the appropiate save func\n",
    "    if save_format == 'h5':\n",
    "      __save_as_h5()\n",
    "    elif save_format == 'composite':\n",
    "      __save_as_composite()\n",
    "    else:  # save as a SavedModel\n",
    "      model.save(file_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "save_model(model, \"fire_cnn_classifier\", \"composite\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to create file (unable to open file: name = '../model/fire_cnn_classifier_params.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4x/v139wvxn7513l8vfxg_3kgkc0000gn/T/ipykernel_27999/3933190740.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fire_cnn_classifier\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"composite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/4x/v139wvxn7513l8vfxg_3kgkc0000gn/T/ipykernel_27999/3197842283.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, file_name, save_format)\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0m__save_as_h5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'composite'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0m__save_as_composite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# save as a SavedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4x/v139wvxn7513l8vfxg_3kgkc0000gn/T/ipykernel_27999/3197842283.py\u001b[0m in \u001b[0;36m__save_as_composite\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;34m\"\"\" Saving the model as H5 (for params) + JSON (for the architecture) \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;31m# Save the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{MODEL_DIRECTORY}/{file_name}_params.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0;31m# Save the architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{MODEL_DIRECTORY}/{file_name}_layers.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, **kwds)\u001b[0m\n\u001b[1;32m    505\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 507\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to create file (unable to open file: name = '../model/fire_cnn_classifier_params.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now go ahead and complete the remaining tasks in `project/PROJECT.md` to complete this project successfully! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---\n",
    "---"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}